{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T03:29:42.977396Z","iopub.status.busy":"2024-09-28T03:29:42.976912Z","iopub.status.idle":"2024-09-28T03:29:43.112601Z","shell.execute_reply":"2024-09-28T03:29:43.110640Z","shell.execute_reply.started":"2024-09-28T03:29:42.977324Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'einops'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTemporalConv\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" EEG to Patch Embedding\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'"]}],"source":["import torch\n","import torch.nn as nn\n","from functools import partial\n","from einops import rearrange\n","from timm.models.layers import trunc_normal_\n","\n","class TemporalConv(nn.Module):\n","    \"\"\" EEG to Patch Embedding\n","    \"\"\"\n","    def __init__(self, in_chans=1, out_chans=8):\n","        '''\n","        in_chans: in_chans of nn.Conv2d()\n","        out_chans: out_chans of nn.Conv2d(), determing the output dimension\n","        '''\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_chans, out_chans, kernel_size=(1, 15), stride=(1, 8), padding=(0, 7))\n","        self.gelu1 = nn.GELU()\n","        self.norm1 = nn.GroupNorm(4, out_chans)\n","        self.conv2 = nn.Conv2d(out_chans, out_chans, kernel_size=(1, 3), padding=(0, 1))\n","        self.gelu2 = nn.GELU()\n","        self.norm2 = nn.GroupNorm(4, out_chans)\n","        self.conv3 = nn.Conv2d(out_chans, out_chans, kernel_size=(1, 3), padding=(0, 1))\n","        self.norm3 = nn.GroupNorm(4, out_chans)\n","        self.gelu3 = nn.GELU()\n","\n","    def forward(self, x, **kwargs):\n","        x = rearrange(x, 'B N A T -> B (N A) T')\n","        B, NA, T = x.shape\n","        x = x.unsqueeze(1)\n","        x = self.gelu1(self.norm1(self.conv1(x)))\n","        x = self.gelu2(self.norm2(self.conv2(x)))\n","        x = self.gelu3(self.norm3(self.conv3(x)))\n","        x = rearrange(x, 'B C NA T -> B NA (T C)')\n","        return x\n","\n","class NeuralTransformer(nn.Module):\n","    def __init__(self, EEG_size=1600, patch_size=200, in_chans=1, out_chans=8, num_classes=1000, embed_dim=200, depth=12,\n","                 num_heads=10, mlp_ratio=4., qkv_bias=False, qk_norm=None, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n","                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n","                 use_mean_pooling=True, init_scale=0.001, **kwargs):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","\n","        # To identify whether it is neural tokenizer or neural decoder. \n","        # For the neural decoder, use linear projection (PatchEmbed) to project codebook dimension to hidden dimension.\n","        # Otherwise, use TemporalConv to extract temporal features from EEG signals.\n","        self.patch_embed = TemporalConv(out_chans=out_chans) # if in_chans == 1 else PatchEmbed(EEG_size=EEG_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        self.time_window = EEG_size // patch_size\n","        self.patch_size = patch_size\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        if use_abs_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, 128 + 1, embed_dim), requires_grad=True)\n","        else:\n","            self.pos_embed = None\n","        self.time_embed = nn.Parameter(torch.zeros(1, 16, embed_dim), requires_grad=True)\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        self.rel_pos_bias = None\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.use_rel_pos_bias = use_rel_pos_bias\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_norm=qk_norm, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values, window_size=None)\n","            for i in range(depth)])\n","        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n","        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if self.pos_embed is not None:\n","            trunc_normal_(self.pos_embed, std=.02)\n","        if self.time_embed is not None:\n","            trunc_normal_(self.time_embed, std=.02)\n","        trunc_normal_(self.cls_token, std=.02)\n","        # trunc_normal_(self.mask_token, std=.02)\n","        if isinstance(self.head, nn.Linear):\n","            trunc_normal_(self.head.weight, std=.02)\n","        self.apply(self._init_weights)\n","        self.fix_init_weight()\n","\n","        if isinstance(self.head, nn.Linear):\n","            self.head.weight.data.mul_(init_scale)\n","            self.head.bias.data.mul_(init_scale)\n","\n","    def fix_init_weight(self):\n","        def rescale(param, layer_id):\n","            param.div_(math.sqrt(2.0 * layer_id))\n","\n","        for layer_id, layer in enumerate(self.blocks):\n","            rescale(layer.attn.proj.weight.data, layer_id + 1)\n","            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token', 'time_embed'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x, input_chans=None, return_patch_tokens=False, return_all_tokens=False, **kwargs):\n","        batch_size, n, a, t = x.shape\n","        input_time_window = a if t == self.patch_size else t\n","        x = self.patch_embed(x)\n","\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1) \n","\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        pos_embed_used = self.pos_embed[:, input_chans] if input_chans is not None else self.pos_embed\n","        if self.pos_embed is not None:\n","            pos_embed = pos_embed_used[:, 1:, :].unsqueeze(2).expand(batch_size, -1, input_time_window, -1).flatten(1, 2)\n","            pos_embed = torch.cat((pos_embed_used[:,0:1,:].expand(batch_size, -1, -1), pos_embed), dim=1)\n","            x = x + pos_embed\n","        if self.time_embed is not None:\n","            nc = n if t == self.patch_size else a\n","            time_embed = self.time_embed[:, 0:input_time_window, :].unsqueeze(1).expand(batch_size, nc, -1, -1).flatten(1, 2)\n","            x[:, 1:, :] += time_embed\n","\n","        x = self.pos_drop(x)\n","        \n","        for blk in self.blocks:\n","            x = blk(x, rel_pos_bias=None)\n","        \n","        x = self.norm(x)\n","        if self.fc_norm is not None:\n","            if return_all_tokens:\n","                return self.fc_norm(x)\n","            t = x[:, 1:, :]\n","            if return_patch_tokens:\n","                return self.fc_norm(t)\n","            else:\n","                return self.fc_norm(t.mean(1))\n","        else:\n","            if return_all_tokens:\n","                return x\n","            elif return_patch_tokens:\n","                return x[:, 1:]\n","            else:\n","                return x[:, 0]\n","\n","    def forward(self, x, input_chans=None, return_patch_tokens=False, return_all_tokens=False, **kwargs):\n","        '''\n","        x: [batch size, number of electrodes, number of patches, patch size]\n","        For example, for an EEG sample of 4 seconds with 64 electrodes, x will be [batch size, 64, 4, 200]\n","        '''\n","        x = self.forward_features(x, input_chans=input_chans, return_patch_tokens=return_patch_tokens, return_all_tokens=return_all_tokens, **kwargs)\n","        x = self.head(x)\n","        return x\n","\n","    def forward_intermediate(self, x, layer_id=12, norm_output=False):\n","        x = self.patch_embed(x)\n","        batch_size, seq_len, _ = x.size()\n","\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        if self.pos_embed is not None:\n","            pos_embed = self.pos_embed[:, 1:, :].unsqueeze(2).expand(batch_size, -1, self.time_window, -1).flatten(1, 2)\n","            pos_embed = torch.cat((self.pos_embed[:,0:1,:].expand(batch_size, -1, -1), pos_embed), dim=1)\n","            x = x + pos_embed\n","        if self.time_embed is not None:\n","            time_embed = self.time_embed.unsqueeze(1).expand(batch_size, 62, -1, -1).flatten(1, 2)\n","            x[:, 1:, :] += time_embed\n","        x = self.pos_drop(x)\n","\n","        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n","        if isinstance(layer_id, list):\n","            output_list = []\n","            for l, blk in enumerate(self.blocks):\n","                x = blk(x, rel_pos_bias=rel_pos_bias)\n","                # use last norm for all intermediate layers\n","                if l in layer_id:\n","                    if norm_output:\n","                        x_norm = self.fc_norm(self.norm(x[:, 1:]))\n","                        output_list.append(x_norm)\n","                    else:\n","                        output_list.append(x[:, 1:])\n","            return output_list\n","        elif isinstance(layer_id, int):\n","            for l, blk in enumerate(self.blocks):\n","                if l < layer_id:\n","                    x = blk(x, rel_pos_bias=rel_pos_bias)\n","                elif l == layer_id:\n","                    x = blk.norm1(x)\n","                else:\n","                    break\n","            return x[:, 1:]\n","    \n","    def get_intermediate_layers(self, x, use_last_norm=False):\n","        x = self.patch_embed(x)\n","        batch_size, seq_len, _ = x.size()\n","\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1) \n","        x = torch.cat((cls_tokens, x), dim=1)\n","        if self.pos_embed is not None:\n","            pos_embed = self.pos_embed[:, 1:, :].unsqueeze(2).expand(batch_size, -1, self.time_window, -1).flatten(1, 2)\n","            pos_embed = torch.cat((self.pos_embed[:,0:1,:].expand(batch_size, -1, -1), pos_embed), dim=1)\n","            x = x + pos_embed\n","        if self.time_embed is not None:\n","            time_embed = self.time_embed.unsqueeze(1).expand(batch_size, 62, -1, -1).flatten(1, 2)\n","            x[:, 1:, :] += time_embed\n","        x = self.pos_drop(x)\n","\n","        features = []\n","        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n","        for blk in self.blocks:\n","            x = blk(x, rel_pos_bias)\n","            if use_last_norm:\n","                features.append(self.norm(x))\n","            else:\n","                features.append(x)\n","\n","        return features\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T03:30:10.329717Z","iopub.status.busy":"2024-09-28T03:30:10.329151Z","iopub.status.idle":"2024-09-28T03:30:10.355877Z","shell.execute_reply":"2024-09-28T03:30:10.354749Z","shell.execute_reply.started":"2024-09-28T03:30:10.329671Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(\n","            self, dim, num_heads=8, qkv_bias=False, qk_norm=None, qk_scale=None, attn_drop=0.,\n","            proj_drop=0., window_size=None, attn_head_dim=None):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        if attn_head_dim is not None:\n","            head_dim = attn_head_dim\n","        all_head_dim = head_dim * self.num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n","        if qkv_bias:\n","            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n","            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n","        else:\n","            self.q_bias = None\n","            self.v_bias = None\n","\n","        if qk_norm is not None:\n","            self.q_norm = qk_norm(head_dim)\n","            self.k_norm = qk_norm(head_dim)\n","        else:\n","            self.q_norm = None\n","            self.k_norm = None\n","\n","        if window_size:\n","            self.window_size = window_size\n","            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","            # cls to token & token 2 cls & cls to cls\n","\n","            # get pair-wise relative position index for each token inside the window\n","            coords_h = torch.arange(window_size[0])\n","            coords_w = torch.arange(window_size[1])\n","            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n","            relative_coords[:, :, 1] += window_size[1] - 1\n","            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n","            relative_position_index = \\\n","                torch.zeros(size=(window_size[0] * window_size[1] + 1, ) * 2, dtype=relative_coords.dtype)\n","            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","            relative_position_index[0, 0:] = self.num_relative_distance - 3\n","            relative_position_index[0:, 0] = self.num_relative_distance - 2\n","            relative_position_index[0, 0] = self.num_relative_distance - 1\n","\n","            self.register_buffer(\"relative_position_index\", relative_position_index)\n","        else:\n","            self.window_size = None\n","            self.relative_position_bias_table = None\n","            self.relative_position_index = None\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(all_head_dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x, rel_pos_bias=None, return_attention=False, return_qkv=False):\n","        B, N, C = x.shape\n","        qkv_bias = None\n","        if self.q_bias is not None:\n","            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n","        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple) (B, H, N, C)\n","        if self.q_norm is not None:\n","            q = self.q_norm(q).type_as(v)\n","        if self.k_norm is not None:\n","            k = self.k_norm(k).type_as(v)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        if self.relative_position_bias_table is not None:\n","            relative_position_bias = \\\n","                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","                    self.window_size[0] * self.window_size[1] + 1,\n","                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n","            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","            attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if rel_pos_bias is not None:\n","            attn = attn + rel_pos_bias\n","        \n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        if return_attention:\n","            return attn\n","            \n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        if return_qkv:\n","            return x, qkv\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def l2norm(t):\n","    return F.normalize(t, p = 2, dim = -1)\n","\n","def ema_inplace(moving_avg, new, decay):\n","    moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))\n","\n","def sample_vectors(samples, num):\n","    num_samples, device = samples.shape[0], samples.device\n","\n","    if num_samples >= num:\n","        indices = torch.randperm(num_samples, device = device)[:num]\n","    else:\n","        indices = torch.randint(0, num_samples, (num,), device = device)\n","\n","    return samples[indices]\n","\n","def kmeans(samples, num_clusters, num_iters = 10, use_cosine_sim = False):\n","    dim, dtype, device = samples.shape[-1], samples.dtype, samples.device\n","\n","    means = sample_vectors(samples, num_clusters)\n","\n","    for _ in range(num_iters):\n","        if use_cosine_sim:\n","            dists = samples @ means.t()\n","        else:\n","            diffs = rearrange(samples, 'n d -> n () d') \\\n","                    - rearrange(means, 'c d -> () c d')\n","            dists = -(diffs ** 2).sum(dim = -1)\n","\n","        buckets = dists.max(dim = -1).indices\n","        bins = torch.bincount(buckets, minlength = num_clusters)\n","        zero_mask = bins == 0\n","        bins_min_clamped = bins.masked_fill(zero_mask, 1)\n","\n","        new_means = buckets.new_zeros(num_clusters, dim, dtype = dtype)\n","        new_means.scatter_add_(0, repeat(buckets, 'n -> n d', d = dim), samples)\n","        new_means = new_means / bins_min_clamped[..., None]\n","\n","        if use_cosine_sim:\n","            new_means = l2norm(new_means)\n","\n","        means = torch.where(zero_mask[..., None], means, new_means)\n","\n","    return means, bins\n","\n","\n","class EmbeddingEMA(nn.Module):\n","    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5, kmeans_init=True, codebook_init_path=''):\n","        super().__init__()\n","        self.num_tokens = num_tokens\n","        self.codebook_dim = codebook_dim\n","        self.decay = decay\n","        self.eps = eps \n","        if codebook_init_path == '':   \n","            if not kmeans_init:\n","                weight = torch.randn(num_tokens, codebook_dim)\n","                weight = l2norm(weight)\n","            else:\n","                weight = torch.zeros(num_tokens, codebook_dim)\n","            self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n","        else:\n","            print(f\"load init codebook weight from {codebook_init_path}\")\n","            codebook_ckpt_weight = torch.load(codebook_init_path, map_location='cpu')\n","            weight = codebook_ckpt_weight.clone()\n","            self.register_buffer('initted', torch.Tensor([True]))\n","            \n","        self.weight = nn.Parameter(weight, requires_grad = False)\n","        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False)\n","        self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False)\n","        # self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n","        self.update = True\n","\n","    @torch.jit.ignore\n","    def init_embed_(self, data):\n","        if self.initted:\n","            return\n","        print(\"Performing Kemans init for codebook\")\n","        embed, cluster_size = kmeans(data, self.num_tokens, 10, use_cosine_sim = True)\n","        self.weight.data.copy_(embed)\n","        self.cluster_size.data.copy_(cluster_size)\n","        self.initted.data.copy_(torch.Tensor([True]))\n","        \n","    def forward(self, embed_id):\n","        return F.embedding(embed_id, self.weight)\n","\n","    def cluster_size_ema_update(self, new_cluster_size):\n","        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n","\n","    def embed_avg_ema_update(self, new_embed_avg): \n","        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n","\n","    def weight_update(self, num_tokens):\n","        n = self.cluster_size.sum()\n","        smoothed_cluster_size = (\n","                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n","            )\n","        #normalize embedding average with smoothed cluster size\n","        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n","        self.weight.data.copy_(embed_normalized)   \n","\n","def norm_ema_inplace(moving_avg, new, decay):\n","    moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))\n","    moving_avg.data.copy_(l2norm(moving_avg.data))\n","\n","class NormEMAVectorQuantizer(nn.Module):\n","    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5, \n","                statistic_code_usage=True, kmeans_init=False, codebook_init_path=''):\n","        super().__init__()\n","        self.codebook_dim = embedding_dim\n","        self.num_tokens = n_embed\n","        self.beta = beta\n","        self.decay = decay\n","        \n","        # learnable = True if orthogonal_reg_weight > 0 else False\n","        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps, kmeans_init, codebook_init_path)\n","        \n","        self.statistic_code_usage = statistic_code_usage\n","        if statistic_code_usage:\n","            self.register_buffer('cluster_size', torch.zeros(n_embed))\n","        \n","        self.all_reduce_fn = nn.Identity()\n","    \n","    def reset_cluster_size(self, device):\n","        if self.statistic_code_usage:\n","            self.register_buffer('cluster_size', torch.zeros(self.num_tokens))\n","            self.cluster_size = self.cluster_size.to(device)\n","\n","    def forward(self, z):\n","        # reshape z -> (batch, height, width, channel) and flatten\n","        #z, 'b c h w -> b h w c'\n","        z = rearrange(z, 'b c h w -> b h w c')\n","        z = l2norm(z)\n","        z_flattened = z.reshape(-1, self.codebook_dim)\n","        self.embedding.init_embed_(z_flattened)\n","        \n","        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n","            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n","            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n'\n","        \n","        encoding_indices = torch.argmin(d, dim=1)\n","\n","        z_q = self.embedding(encoding_indices).view(z.shape)\n","        \n","        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)     \n","        \n","        if not self.training:\n","            with torch.no_grad():\n","                cluster_size = encodings.sum(0)\n","                self.all_reduce_fn(cluster_size)\n","                ema_inplace(self.cluster_size, cluster_size, self.decay)\n","        \n","        if self.training and self.embedding.update:\n","            #EMA cluster size\n","\n","            bins = encodings.sum(0)\n","            self.all_reduce_fn(bins)\n","\n","            # self.embedding.cluster_size_ema_update(bins)\n","            ema_inplace(self.cluster_size, bins, self.decay)\n","\n","            zero_mask = (bins == 0)\n","            bins = bins.masked_fill(zero_mask, 1.)\n","\n","            embed_sum = z_flattened.t() @ encodings\n","            self.all_reduce_fn(embed_sum)\n","                        \n","            embed_normalized = (embed_sum / bins.unsqueeze(0)).t()\n","            embed_normalized = l2norm(embed_normalized)\n","            \n","            embed_normalized = torch.where(zero_mask[..., None], self.embedding.weight,\n","                                           embed_normalized)\n","\n","            norm_ema_inplace(self.embedding.weight, embed_normalized, self.decay)\n","\n","        # compute loss for embedding\n","        loss = self.beta * F.mse_loss(z_q.detach(), z) \n","        \n","        # preserve gradients\n","        z_q = z + (z_q - z).detach()\n","\n","        # reshape back to match original input shape\n","        #z_q, 'b h w c -> b c h w'\n","        z_q = rearrange(z_q, 'b h w c -> b c h w')\n","        return z_q, loss, encoding_indices\n","    \n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T03:30:26.498022Z","iopub.status.busy":"2024-09-28T03:30:26.497002Z","iopub.status.idle":"2024-09-28T03:30:26.509994Z","shell.execute_reply":"2024-09-28T03:30:26.508546Z","shell.execute_reply.started":"2024-09-28T03:30:26.497974Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_norm=None, qk_scale=None, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm, init_values=None, window_size=None, attn_head_dim=None):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, bias=qkv_bias)\n","        self.drop_path = nn.Identity() if drop_path == 0 else nn.Dropout(drop_path)\n","        self.norm2 = norm_layer(dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, int(dim * mlp_ratio)),\n","            nn.GELU(),\n","            nn.Linear(int(dim * mlp_ratio), dim),\n","            nn.Dropout(drop)\n","        )\n","\n","    def forward(self, x, rel_pos_bias=None):\n","        x = x + self.drop_path(self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0])\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        return x\n","    \n","    \n","class PatchEmbed(nn.Module):\n","    \"\"\" EEG to Patch Embedding\n","    \"\"\"\n","    def __init__(self, EEG_size=2000, patch_size=200, in_chans=1, embed_dim=200):\n","        super().__init__()\n","        # EEG_size = to_2tuple(EEG_size)\n","        # patch_size = to_2tuple(patch_size)\n","        num_patches = 62 * (EEG_size // patch_size)\n","        self.patch_shape = (1, EEG_size // patch_size)\n","        self.EEG_size = EEG_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=(1, patch_size), stride=(1, patch_size))\n","\n","    def forward(self, x, **kwargs):\n","        B, C, H, W = x.shape\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-28T03:31:17.907251Z","iopub.status.busy":"2024-09-28T03:31:17.906838Z","iopub.status.idle":"2024-09-28T03:31:17.947689Z","shell.execute_reply":"2024-09-28T03:31:17.946661Z","shell.execute_reply.started":"2024-09-28T03:31:17.907211Z"},"trusted":true},"outputs":[],"source":["class VQNSP(nn.Module):\n","    def __init__(self,\n","                 encoder_config,\n","                 decoder_config,\n","                 n_embed=8192, \n","                 embed_dim=32,\n","                 decay=0.99,\n","                 quantize_kmeans_init=True,\n","                 decoder_out_dim=200,\n","                 smooth_l1_loss = False,\n","                 **kwargs\n","                 ):\n","        super().__init__()\n","        print(kwargs)\n","        if decoder_config['in_chans'] != embed_dim:\n","            print(f\"Rewrite the in_chans in decoder from {decoder_config['in_chans']} to {embed_dim}\")\n","            decoder_config['in_chans'] = embed_dim\n","\n","        # encoder & decode params\n","        self.encoder = NeuralTransformer(**encoder_config)\n","\n","        self.decoder = NeuralTransformer(**decoder_config)\n","                \n","        self.quantize = NormEMAVectorQuantizer(\n","            n_embed=n_embed, embedding_dim=embed_dim, beta=1.0, kmeans_init=quantize_kmeans_init, decay=decay,\n","        )\n","        \n","        self.patch_size = encoder_config['patch_size']\n","        self.token_shape = (62, encoder_config['EEG_size'] // self.patch_size)\n","\n","        self.decoder_out_dim = decoder_out_dim\n","\n","        # task layer\n","        self.encode_task_layer = nn.Sequential(\n","            nn.Linear(encoder_config['embed_dim'], encoder_config['embed_dim']),\n","            nn.Tanh(),\n","            nn.Linear(encoder_config['embed_dim'], embed_dim) # for quantize\n","        )\n","        self.decode_task_layer = nn.Sequential(\n","            nn.Linear(decoder_config['embed_dim'], decoder_config['embed_dim']),\n","            nn.Tanh(),\n","            nn.Linear(decoder_config['embed_dim'], self.decoder_out_dim),\n","        )\n","        self.decode_task_layer_angle = nn.Sequential(\n","            nn.Linear(decoder_config['embed_dim'], decoder_config['embed_dim']),\n","            nn.Tanh(),\n","            nn.Linear(decoder_config['embed_dim'], self.decoder_out_dim),\n","        )\n","\n","        self.kwargs = kwargs\n","        \n","        self.encode_task_layer.apply(self._init_weights)\n","        self.decode_task_layer.apply(self._init_weights)\n","        self.decode_task_layer_angle.apply(self._init_weights)\n","\n","        self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss\n","    \n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","            \n","\n","    @property\n","    def device(self):\n","        return self.decoder.cls_token.device\n","        \n","    def get_number_of_tokens(self):\n","        return self.quantize.n_e\n","\n","    def get_tokens(self, data, input_chans=None, **kwargs):\n","        quantize, embed_ind, loss = self.encode(data, input_chans=input_chans)\n","        output = {}\n","        output['token'] = embed_ind.view(data.shape[0], -1)\n","        output['input_img'] = data\n","        output['quantize'] = rearrange(quantize, 'b d a c -> b (a c) d')\n","\n","        return output\n","\n","    def encode(self, x, input_chans=None):\n","        batch_size, n, a, t = x.shape\n","        encoder_features = self.encoder(x, input_chans, return_patch_tokens=True)\n","\n","        with torch.cuda.amp.autocast(enabled=False):\n","            to_quantizer_features = self.encode_task_layer(encoder_features.type_as(self.encode_task_layer[-1].weight))\n","\n","        N = to_quantizer_features.shape[1]\n","        h, w = n, N // n\n","\n","        to_quantizer_features = rearrange(to_quantizer_features, 'b (h w) c -> b c h w', h=h, w=w) # reshape for quantizer\n","        quantize, loss, embed_ind = self.quantize(to_quantizer_features)\n","\n","        return quantize, embed_ind, loss\n","    \n","    def decode(self, quantize, input_chans=None, **kwargs):\n","        # reshape tokens to feature maps for patch embed in decoder\n","        quantize = rearrange(quantize, 'b (h w) c -> b c h w', h=self.token_shape[0], w=self.token_shape[1])\n","        decoder_features = self.decoder(quantize, input_chans, return_patch_tokens=True)\n","        rec = self.decode_task_layer(decoder_features)\n","        rec_angle = self.decode_task_layer_angle(decoder_features)\n","        return rec, rec_angle\n","    \n","    def get_codebook_indices(self, x, input_chans=None, **kwargs):\n","        # for LaBraM pre-training\n","        return self.get_tokens(x, input_chans, **kwargs)['token']\n","    \n","    def calculate_rec_loss(self, rec, target):\n","        target = rearrange(target, 'b n a c -> b (n a) c')\n","        rec_loss = self.loss_fn(rec, target)\n","        return rec_loss\n","    \n","    def std_norm(self, x):\n","        mean = torch.mean(x, dim=(1, 2, 3), keepdim=True)\n","        std = torch.std(x, dim=(1, 2, 3), keepdim=True)\n","        x = (x - mean) / std\n","        return x\n","\n","    def forward(self, x, input_chans=None, **kwargs):\n","        \"\"\"\n","        x: shape [B, N, T]\n","        \"\"\"\n","\n","        x = rearrange(x, 'B N (A T) -> B N A T', T=200)\n","        x_fft = torch.fft.fft(x, dim=-1)\n","        amplitude = torch.abs(x_fft)\n","        amplitude = self.std_norm(amplitude)\n","        angle = torch.angle(x_fft)     # change this angle to be the l2 norm distance of vector \n","        angle = self.std_norm(angle)\n","\n","        quantize, embed_ind, emb_loss = self.encode(x, input_chans)\n","        \n","        xrec, xrec_angle = self.decode(quantize, input_chans)\n","        rec_loss = self.calculate_rec_loss(xrec, amplitude)\n","        rec_angle_loss = self.calculate_rec_loss(xrec_angle, angle)\n","        loss = emb_loss + rec_loss + rec_angle_loss\n","\n","        log = {}\n","        split=\"train\" if self.training else \"val\"\n","        log[f'{split}/quant_loss'] = emb_loss.detach().mean()\n","        log[f'{split}/rec_loss'] = rec_loss.detach().mean()\n","        log[f'{split}/rec_angle_loss'] = rec_angle_loss.detach().mean()\n","        log[f'{split}/total_loss'] = loss.detach().mean()\n","\n","        return loss, log\n","\n","def get_model_default_params():\n","    return dict(EEG_size=1600, patch_size=200, in_chans=1, num_classes=1000, embed_dim=200, depth=12, num_heads=10,  \n","                             mlp_ratio=4., qkv_bias=True,  qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., \n","                             norm_layer=partial(nn.LayerNorm, eps=1e-6), init_values=0., use_abs_pos_emb=True, \n","                             use_rel_pos_bias=False, use_shared_rel_pos_bias=False, use_mean_pooling=True, init_scale=0.001)\n","\n","\n","def vqnsp_encoder(pretrained=False, pretrained_weight=None, as_tokenzer=False, EEG_size=1600, \n","                                            n_code=8192, code_dim=32, **kwargs):\n","    encoder_config, decoder_config = get_model_default_params(), get_model_default_params()\n","\n","    # encoder settings\n","    encoder_config['EEG_size'] = EEG_size\n","    encoder_config['num_classes'] = 0\n","    # decoder settings\n","    decoder_config['EEG_size'] = EEG_size // decoder_config['patch_size']\n","    decoder_config['patch_size'] = 1\n","    decoder_config['in_chans'] = code_dim\n","    decoder_config['num_classes'] = 0\n","    decoder_config['depth'] = 3\n","    decoder_out_dim = 200\n","\n","    model = VQNSP(encoder_config, decoder_config, n_code, code_dim, \n","                 decoder_out_dim=decoder_out_dim, **kwargs)\n","\n","    if as_tokenzer:\n","        assert pretrained\n","        assert pretrained_weight is not None\n","\n","        if pretrained_weight.startswith('https'):\n","            weights = torch.hub.load_state_dict_from_url(pretrained_weight, map_location='cpu', check_hash=True)\n","        else:\n","            weights = torch.load(pretrained_weight, map_location='cpu')\n","            \n","        if 'model' in weights:\n","            weights = weights['model']\n","        else:\n","            weights = weights[\"state_dict\"]\n","        keys = list(weights.keys())\n","        \n","        for k in keys:\n","            if k.startswith(\"loss\") or k.startswith(\"teacher\") or k.startswith(\"scaling\"):\n","                del weights[k]\n","        model.load_state_dict(weights)\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-27T03:46:20.112298Z","iopub.status.busy":"2024-09-27T03:46:20.111785Z","iopub.status.idle":"2024-09-27T03:46:20.524205Z","shell.execute_reply":"2024-09-27T03:46:20.522455Z","shell.execute_reply.started":"2024-09-27T03:46:20.112253Z"},"trusted":true},"outputs":[],"source":["# train the model\n","import argparse\n","import datetime\n","import numpy as np\n","import time\n","import torch\n","import torch.backends.cudnn as cudnn\n","import json\n","import os\n","from pathlib import Path\n","from torch.utils.data import DataLoader, Dataset\n","import h5py\n","from tensorboardX import SummaryWriter\n","\n","class EEGDataset(Dataset):\n","    def __init__(self, h5_dir):\n","        self.h5_files = list(Path(h5_dir).glob('*.h5'))\n","\n","    def __len__(self):\n","        return len(self.h5_files)\n","\n","    def __getitem__(self, idx):\n","        with h5py.File(self.h5_files[idx], 'r') as f:\n","            data = f['eeg'][:]\n","            ch_names = f['ch_names'][:]\n","        return torch.tensor(data, dtype=torch.float32), ch_names\n","\n","def train_one_epoch(model, data_loader, optimizer, device, epoch, loss_scaler, log_writer=None):\n","    model.train()\n","    for batch in data_loader:\n","        data, _ = batch\n","        EEG = data.float().to(device, non_blocking=True) / 100\n","        loss, log_loss = model(EEG, input_chans=1) # TODO: set the input_channels for this data\n","        loss_value = loss.item()\n","        \n","        optimizer.zero_grad()\n","        optimizer.step()\n","        if log_writer:\n","            log_writer.add_scalar('train_loss', loss_value, epoch)\n","\n","def main(args):\n","    device = torch.device(args.device)\n","    cudnn.benchmark = True\n","\n","    model = vqnsp_encoder()\n","    model.to(device)\n","\n","    dataset = EEGDataset(args.data_dir)\n","    data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n","    loss_scaler = None\n","\n","    if args.log_dir:\n","        log_writer = SummaryWriter(log_dir=args.log_dir)\n","    else:\n","        log_writer = None\n","\n","    print(f\"Start training for {args.epochs} epochs\")\n","    start_time = time.time()\n","    for epoch in range(args.epochs):\n","        train_one_epoch(model, data_loader, optimizer, device, epoch, loss_scaler, log_writer)\n","        if args.output_dir:\n","            torch.save(model.state_dict(), os.path.join(args.output_dir, f\"checkpoint_{epoch}.pth\"))\n","\n","    total_time = time.time() - start_time\n","    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","    print('Training time {}'.format(total_time_str))\n","\n","# running with temp files\n","class Args:\n","    batch_size = 64\n","    epochs = 100\n","    lr = 5e-4\n","    data_dir = 'TOADD'\n","    output_dir = 'TOADD'\n","    log_dir = 'TOADD'\n","    device = 'cuda'\n","    num_workers = 10\n","\n","args = Args()\n","if args.output_dir:\n","    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n","main(args)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3469109,"sourceId":6062067,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
